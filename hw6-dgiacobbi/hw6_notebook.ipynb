{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c50355",
   "metadata": {},
   "source": [
    "David Giacobbi, CPSC 322, Fall 2023, Notebook for HW-6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3952",
   "metadata": {},
   "source": [
    "# 1. Load libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05155db4",
   "metadata": {},
   "source": [
    " Import the data table and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed147760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_table import *\n",
    "from data_learn import *\n",
    "from data_eval import *\n",
    "from data_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6208afb",
   "metadata": {},
   "source": [
    "# 2. Auto MPG Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de2c0",
   "metadata": {},
   "source": [
    "Load and clean auto data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0decd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "\n",
    "# Rmove duplicate rows\n",
    "auto = remove_duplicates(auto)\n",
    "\n",
    "# Remove rows with missing values in any columns\n",
    "auto = remove_missing(auto, auto.columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92f7bad",
   "metadata": {},
   "source": [
    "## Step 1: k-NN versus Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretize the mpg value in the auto table using three equal-width bins. Normalize the weight (wt) and displacement (disp) attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize the mpg value into three equal-width bins\n",
    "discretize(auto, 'mpg', [20, 30])\n",
    "\n",
    "# Normalize weight and displacement attributes\n",
    "normalize(auto, 'weight')\n",
    "normalize(auto, 'disp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate knn using stratified k-fold cross validation (i.e., your knn_stratified() function) to predict mpg labels using 10 folds, a knn k-value of 7, majority voting, and only the weight and displacement attributes (as numeric columns). Display the resulting confusion matrix. Compute accuracy, precision, recall, and the f-measure over the resulting confusion matrix and display each. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1  130   18    1\n",
      "       2   16   99    7\n",
      "       3    0   11   25\n",
      "\n",
      "Average Accuracy: 0.8849077090119435\n",
      "\n",
      "Average Precision: 0.8071414054932889\n",
      "\n",
      "Average Recall: 0.79280102525234\n",
      "\n",
      "Macro Average f-measure: 0.7993312044542701\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a stratified \n",
    "k_fold_matrix = knn_stratified(auto, 10, 'mpg', majority_vote, 7, ['weight', 'disp'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat step 3 using the same parameters but using naive-bayes instead of knn (i.e., your naive_bayes_stratified() function). Be sure to use weight and displacement as continuous attributes. Display the resulting confusion matrix. Compute accraccy, precision, recall, and the f-measure over the resulting confusion matrix and display each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1  133   15    1\n",
      "       2   16   73   33\n",
      "       3    0    2   34\n",
      "\n",
      "Average Accuracy: 0.8545059717698154\n",
      "\n",
      "Average Precision: 0.7345761869251802\n",
      "\n",
      "Average Recall: 0.8118075166155263\n",
      "\n",
      "Macro Average f-measure: 0.7450476162645341\n"
     ]
    }
   ],
   "source": [
    "# Evaluate a stratified \n",
    "k_fold_matrix = naive_bayes_stratified(auto, 10, 'mpg', ['weight', 'disp'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the results from 4,5 and 6,7. Write down your thoughts and observations concerning the results. \n",
    "\n",
    "Although the naive bayes and knn approach have very similar evaluation metrics, the f-measure provides the best overarching evaluation of the classifiers effectiveness across any dataset. In this case, knn appeared to perform at a 5% better classifier than naive bayes. I think this is because knn uses Euclidean distances to classify, and its formula appears to be more accurate for the continuous values in comparison to the gaussian density of naive bayes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e4dbb",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation with Auto MPG Data Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d39f9ee",
   "metadata": {},
   "source": [
    "Experiment with different numbers of folds, different attributes, different knn k-values, and so on, to find the parameters that work best for each approach (knn verus naive bayes) on the mpg data set. \n",
    "\n",
    "### Experimentation with stratified kNN\n",
    "\n",
    "1. k-folds: 6, majority_vote, k: 3, ['weight', 'disp', 'cyls', 'accl']\n",
    "1. k-folds: 12, weighted_vote, k: 5, ['weight', 'disp', 'cyls', 'accl', 'hp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1  134   15    0\n",
      "       2   17   95   10\n",
      "       3    0   13   23\n",
      "\n",
      "Average Accuracy: 0.8805646036916395\n",
      "\n",
      "Average Precision: 0.7855815463633263\n",
      "\n",
      "Average Recall: 0.7723020908464852\n",
      "\n",
      "Macro Average f-measure: 0.7785034013605442\n"
     ]
    }
   ],
   "source": [
    "# k-folds: 6, majority_vote, k: 3, ['weight', 'disp', 'cyls', 'accl']\n",
    "normalize(auto, 'cyls')\n",
    "normalize(auto, 'accl')\n",
    "k_fold_matrix = knn_stratified(auto, 6, 'mpg', majority_vote, 3, ['weight', 'disp', 'cyls', 'accl'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1  133   15    1\n",
      "       2   16   91   15\n",
      "       3    0    6   30\n",
      "\n",
      "Average Accuracy: 0.8849077090119435\n",
      "\n",
      "Average Precision: 0.7857637875693028\n",
      "\n",
      "Average Recall: 0.8239508074473418\n",
      "\n",
      "Macro Average f-measure: 0.8007008481717928\n"
     ]
    }
   ],
   "source": [
    "# k-folds: 12, weighted_vote, k: 5, ['weight', 'disp', 'hp', 'cyls', 'accl']\n",
    "normalize(auto, 'hp')\n",
    "k_fold_matrix = knn_stratified(auto, 12, 'mpg', weighted_vote, 5, ['weight', 'disp', 'hp', 'cyls', 'accl'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimentation with Stratified Naive Bayes\n",
    "\n",
    "1. k-folds: 12, ['weight', 'disp', 'hp', 'cyls', 'accl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1  128   17    4\n",
      "       2   12   21   89\n",
      "       3    0    0   36\n",
      "\n",
      "Average Accuracy: 0.7350705754614549\n",
      "\n",
      "Average Precision: 0.5819956868916477\n",
      "\n",
      "Average Recall: 0.6770638500751825\n",
      "\n",
      "Macro Average f-measure: 0.5282255950508545\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = naive_bayes_stratified(auto, 12, 'mpg', ['weight', 'disp', 'hp', 'cyls', 'accl'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get a best idea of things that worked, I started with analyzing the stratified kNN. I noticed that increasing the number of folds as well as decreasing the k value and using weighted voting slightly increased the classifier's effectiveness. I wanted to add more attributes pertaining to the engine as larger engines typically require more power to work. However, when I used these changes on the Naive Bayes classifier, I noticed that the more attributes greatly impacted its accuracy. The more attributes and folded given to Naive Bayes appeared to have a very negative effect. My inference is that more continuous values in a Naive Bayes greatly decreases overall f-measure as it is based on the idea that the continuous value is in a perfect normal distribution, which is not typically the case. This is important to note when choosing continuous values for a Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5c32c",
   "metadata": {},
   "source": [
    "# 2. Titanic Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8cfc4",
   "metadata": {},
   "source": [
    "Load the titanic data set below. The attributes are *class*, *age*, *gender*, and *survival*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2916b12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = DataTable(['class', 'age', 'gender', 'survival'])\n",
    "titanic.load('titanic.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the titanic data set, predict survival using both knn and naive bayes via stratified k-fold cross validation using each of the other attributes as categorical features. For both, use 4 folds. For knn use a k-value of 7 and majority voting. As above, show the resulting confusion matrix for both along with accuraccy, precision, recall, and f measure results.\n",
    "\n",
    "### k-NN Stratified Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      yes    no\n",
      "--------  -----  ----\n",
      "yes        1490     0\n",
      "no          711     0\n",
      "\n",
      "Average Accuracy: 0.6769650159018628\n",
      "\n",
      "Average Precision: 0.8384825079509314\n",
      "\n",
      "Average Recall: 0.5\n",
      "\n",
      "Macro Average f-measure: 0.4036846383094012\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = knn_stratified(titanic, 4, 'survival', majority_vote, 7, [], nom_cols=['class', 'age', 'gender'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(titanic, ['survival']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(titanic, ['survival']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Stratified Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual      yes    no\n",
      "--------  -----  ----\n",
      "yes        1364   126\n",
      "no          360   351\n",
      "\n",
      "Average Accuracy: 0.7791912766924125\n",
      "\n",
      "Average Precision: 0.7635161756336732\n",
      "\n",
      "Average Recall: 0.7045535638433438\n",
      "\n",
      "Macro Average f-measure: 0.7198478248571589\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = naive_bayes_stratified(titanic, 4, 'survival', [], cat_cols=['class', 'age', 'gender'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(titanic, ['survival']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(titanic, ['survival']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c645af2",
   "metadata": {},
   "source": [
    "# 3. Student Stress Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f68ce9c",
   "metadata": {},
   "source": [
    "Load the student stress data set below. The attributes are given below in column order, where the short name to use is given in parenthesis: \n",
    "1. sleep_quality (sleep)\n",
    "2. living_conditions (living)\n",
    "3. basic_needs (basics)\n",
    "4. academic_performance (academic)\n",
    "5. study_load (study)\n",
    "6. future_career_concerns (career)\n",
    "7. social_support (social)\n",
    "8. extracurricular_activities (extra)\n",
    "9. stress_level (stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2be851a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_stress = DataTable(['sleep', 'living', 'basics', 'academic', 'study', 'career', 'social', 'extra', 'stress'])\n",
    "student_stress.load('student-stress.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7a367e",
   "metadata": {},
   "source": [
    "## Step 1: Initial kNN and Naive Bayes Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db71c6bf",
   "metadata": {},
   "source": [
    "Use stratified k-fold cross validation to evaluate knn and naive bayes for predicting student stress level (the stress attribute) using the other table attributes as categorical values. For both evaluations use 10 folds, and for knn use a k-value of 7 and majority voting. Give your resulting confusion matrices as well as accuracy, precision, recall, and f-measure values.\n",
    "\n",
    "### kNN Stratified Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    0\n",
      "--------  ---  ---  ---\n",
      "       1  231    1  126\n",
      "       2    0  300   69\n",
      "       0    0    0  373\n",
      "\n",
      "Average Accuracy: 0.8812121212121212\n",
      "\n",
      "Average Precision: 0.8844559605696193\n",
      "\n",
      "Average Recall: 0.8194198422431151\n",
      "\n",
      "Macro Average f-measure: 0.8242254462402888\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = knn_stratified(student_stress, 10, 'stress', majority_vote, 7, [], nom_cols=['sleep', 'living', 'basics', 'academic', \n",
    "                                                                                             'study', 'career', 'social', 'extra'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Stratified Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    0\n",
      "--------  ---  ---  ---\n",
      "       1  314   24   20\n",
      "       2   15  332   22\n",
      "       0   16   35  322\n",
      "\n",
      "Average Accuracy: 0.9199999999999999\n",
      "\n",
      "Average Precision: 0.8812883904955516\n",
      "\n",
      "Average Recall: 0.8800315822789683\n",
      "\n",
      "Macro Average f-measure: 0.8802704439782209\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = naive_bayes_stratified(student_stress, 10, 'stress', [], cat_cols=['sleep', 'living', 'basics', 'academic', \n",
    "                                                                                   'study', 'career', 'social', 'extra'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7247cb1",
   "metadata": {},
   "source": [
    "## Step 2: Experimentation with Student Stress Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f127d0d2",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Changes\n",
    "\n",
    "1. k-fold: 14, ['sleep', 'basics', 'career', 'academic']\n",
    "1. k-fold: 14, ['study', 'social', 'extra', 'living']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    0\n",
      "--------  ---  ---  ---\n",
      "       1  314   18   26\n",
      "       2   23  323   23\n",
      "       0   23   23  327\n",
      "\n",
      "Average Accuracy: 0.9175757575757576\n",
      "\n",
      "Average Precision: 0.8764219035495632\n",
      "\n",
      "Average Recall: 0.8763697762239104\n",
      "\n",
      "Macro Average f-measure: 0.8763752385881931\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = naive_bayes_stratified(student_stress, 14, 'stress', [], cat_cols=['sleep', 'basics', 'academic', 'career'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    0\n",
      "--------  ---  ---  ---\n",
      "       1  289   25   44\n",
      "       2   10  334   25\n",
      "       0   12   41  320\n",
      "\n",
      "Average Accuracy: 0.9048484848484848\n",
      "\n",
      "Average Precision: 0.8622941860433078\n",
      "\n",
      "Average Recall: 0.8567734895026345\n",
      "\n",
      "Macro Average f-measure: 0.8575105650032796\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = naive_bayes_stratified(student_stress, 14, 'stress', [], cat_cols=['living', 'study', 'social', 'extra'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Classifier Changes\n",
    "\n",
    "1. k-folds: 12, majority_vote, k: 10, ['sleep', 'living', 'basics', 'career', 'social']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    0\n",
      "--------  ---  ---  ---\n",
      "       1  314   28   16\n",
      "       2    9  345   15\n",
      "       0    8   49  316\n",
      "\n",
      "Average Accuracy: 0.9242424242424243\n",
      "\n",
      "Average Precision: 0.8922796175382918\n",
      "\n",
      "Average Recall: 0.8864131027519031\n",
      "\n",
      "Macro Average f-measure: 0.8871857325188431\n"
     ]
    }
   ],
   "source": [
    "k_fold_matrix = knn_stratified(student_stress, 12, 'stress', majority_vote, 4, [], nom_cols=['sleep', 'living', 'basics', 'academic', \n",
    "                                                                                             'study', 'career', 'social', 'extra'])\n",
    "print(k_fold_matrix)\n",
    "\n",
    "# Compute accuracy, precision, recall\n",
    "acc = []\n",
    "prec = []\n",
    "rec = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    acc.append(accuracy(k_fold_matrix, label))\n",
    "    prec.append(precision(k_fold_matrix, label))\n",
    "    rec.append(recall(k_fold_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "print(\"\\nAverage Precision:\", sum(prec)/len(prec))\n",
    "print(\"\\nAverage Recall:\", sum(rec)/len(rec))\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(student_stress, ['stress']):\n",
    "    cur_recall = recall(k_fold_matrix, label)\n",
    "    cur_precision = precision(k_fold_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "print(\"\\nMacro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the changes that were made, I wanted to take a look at ways to split up the Naive Bayes attributes to see if some of them had a significant weight over the others. In the first test, I selected attributes that appeared to be the best indicators in identifying stress in people. I then placed the remaining ones in a different Naive Bayes classifier. Despite this change, I did not notice a difference. I think that all of the attributes had similar influences and weight, so the more attributes provided actually end up painting a better picture for the model to classify given the three options for stress levels.\n",
    "\n",
    "Using this information, I altered the k value of the kNN classifier. I noticed that the voting scheme was thwarting the classifier's effectiveness. When I increased the k too high, I ended up with a classifier that only wanted to predict 0 stress levels. So, I decreased the k value to four and notcied a very high jump in the kNN's evaluation metrics, ultimately making it the most accurate of my altered models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6390bb",
   "metadata": {},
   "source": [
    "# 4. Issues, Challenges, and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b940",
   "metadata": {},
   "source": [
    "Overall, the assignment did not have too many struggles in the implementation of Naive Bayes. I think that the concept was easier to grasp after already having written a model for kNN. Moreover, the most difficulty I encountered was trusting that my evaluation functions were correctly performing stratification and depicting its results and metrics accurately in the confusion matrix. I think that since there were no python tests to check this, it was difficult to trust that my algorithm was performing what I belived it to be correct simply because it was not throwing any errors when I ran my program. The Jupyter notebook helped me to learn more about the strengths and weaknesses of kNN and Naive Bayes classifiers, which I hope to implement into my own project soon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
