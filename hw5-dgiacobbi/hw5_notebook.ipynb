{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9c50355",
   "metadata": {},
   "source": [
    "David Giacobbi, CPSC 322, Fall 2023, Notebook for HW-5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084b3952",
   "metadata": {},
   "source": [
    "# 1. Load libraries and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05155db4",
   "metadata": {},
   "source": [
    " Import the data table and utility functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed147760",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_table import *\n",
    "from data_learn import *\n",
    "from data_eval import *\n",
    "from data_util import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11de2c0",
   "metadata": {},
   "source": [
    "Load and clean auto data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0decd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, auto.columns())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b5c32c",
   "metadata": {},
   "source": [
    "# 2. Exploring k-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8cfc4",
   "metadata": {},
   "source": [
    "1. Discretize the mpg values in the auto table using three equal-width bins\n",
    "2. Normalize all of the columns except for model and origin\n",
    "3. Create a train and test set using holdout with approximately half of the rows in the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c59f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretize mpg values with three equal-width bins\n",
    "discretize(auto, 'mpg', [20,30])\n",
    "\n",
    "# Normalize all the columns\n",
    "norm_cols = ['cyls','disp','hp','weight','accl']\n",
    "for column in norm_cols:\n",
    "    normalize(auto, column)\n",
    "\n",
    "# Create test and train set with holdout\n",
    "test_set_size = int(auto.row_count() / 2)\n",
    "train_set, test_set = holdout(auto, test_set_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e1b06d",
   "metadata": {},
   "source": [
    "4. Run knn over the train and test set to predict mpg class labels, using majority voting, k=5, the numerical columns cylinders, weight, and acceleration, and no nominal attributes. \n",
    "5. Print the resulting confusion matrix\n",
    "6. Calculate and print the (average) accuracy across mpg labels, and the macro average f-measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab331e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   72    1    0\n",
      "       2   16   37    6\n",
      "       3    0   12    9\n",
      "\n",
      "Average Accuracy: 0.8474945533769063\n",
      "Macro Average f-measure: 0.680663814167413\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = knn_eval(train_set, test_set, majority_vote, 5, 'mpg', ['cyls','weight','accl'])\n",
    "print(confusion_matrix)\n",
    "\n",
    "\n",
    "# Create a list of accuracies for each mpg label\n",
    "acc = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "\n",
    "# Calculate the macro average f-measure\n",
    "f_measures = []\n",
    "for label in distinct_values(auto, ['mpg']):\n",
    "    cur_recall = recall(confusion_matrix, label)\n",
    "    cur_precision = recall(confusion_matrix, label)\n",
    "    f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011eeed4",
   "metadata": {},
   "source": [
    "7. Run steps 1-6 3 times (i.e., put 1-6 in a for loop that iterates three times)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a702cb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   69   11    0\n",
      "       2   12   41    4\n",
      "       3    0    6   10\n",
      "\n",
      "Average Accuracy: 0.8562091503267973\n",
      "Macro Average f-measure: 0.7355994152046783 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   68    6    0\n",
      "       2   10   41    8\n",
      "       3    0    5   15\n",
      "\n",
      "Average Accuracy: 0.8736383442265795\n",
      "Macro Average f-measure: 0.7879447243854024 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   65    8    0\n",
      "       2    9   49    5\n",
      "       3    0    6   11\n",
      "\n",
      "Average Accuracy: 0.8779956427015251\n",
      "Macro Average f-measure: 0.7717491867370997 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run 3 iterations of knn\n",
    "for i in range(3):\n",
    "\n",
    "    # Create test and train set with holdout\n",
    "    test_set_size = int(auto.row_count() / 2)\n",
    "    train_set, test_set = holdout(auto, test_set_size)\n",
    "\n",
    "    # knn evaluation\n",
    "    confusion_matrix = knn_eval(train_set, test_set, majority_vote, 5, 'mpg', ['cyls','weight','accl'])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Create a list of accuracies for each mpg label\n",
    "    acc = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "    # Calculate the macro average f-measure\n",
    "    f_measures = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        cur_recall = recall(confusion_matrix, label)\n",
    "        cur_precision = recall(confusion_matrix, label)\n",
    "        f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "    print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba4223c",
   "metadata": {},
   "source": [
    "8. Redo 7 (as another for loop) that uses weighted voting instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "491d12b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   58    8    1\n",
      "       2   10   51    1\n",
      "       3    0   18    6\n",
      "\n",
      "Average Accuracy: 0.8344226579520697\n",
      "Macro Average f-measure: 0.6460840956507784 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   62   16    0\n",
      "       2    8   44    8\n",
      "       3    0    5   10\n",
      "\n",
      "Average Accuracy: 0.8387799564270152\n",
      "Macro Average f-measure: 0.7316239316239316 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   65    7    0\n",
      "       2   13   46    5\n",
      "       3    0    6   11\n",
      "\n",
      "Average Accuracy: 0.8649237472766885\n",
      "Macro Average f-measure: 0.7561955337690631 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run 3 iterations of knn\n",
    "for i in range(3):\n",
    "\n",
    "    # Create test and train set with holdout\n",
    "    test_set_size = int(auto.row_count() / 2)\n",
    "    train_set, test_set = holdout(auto, test_set_size)\n",
    "\n",
    "    # knn evaluation\n",
    "    confusion_matrix = knn_eval(train_set, test_set, weighted_vote, 5, 'mpg', ['cyls','weight','accl'])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Create a list of accuracies for each mpg label\n",
    "    acc = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "    # Calculate the macro average f-measure\n",
    "    f_measures = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        cur_recall = recall(confusion_matrix, label)\n",
    "        cur_precision = recall(confusion_matrix, label)\n",
    "        f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "    print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b464d2",
   "metadata": {},
   "source": [
    "9. Compare the performance differences, if any, between the results from 6 and 7.\n",
    "\n",
    "Overall, the results from the majority_vote knn evaluation had a slightly higher average accuracy. Moreover, the f-measures for weighted vote also saw a small drop off. Initial analysis on these two methods would argue that the majority vote was the stronger method, and scoring weights caused some predictions to alter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d87a4b",
   "metadata": {},
   "source": [
    "10. Pick two other bin sizes (either equal width or hand-crafted cut-points) for mpg values and redo 6 (as another for loop) for each. Compare the results.\n",
    "\n",
    "Bin Size 1: [22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c34d1d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2\n",
      "--------  ---  ---\n",
      "       1   84    6\n",
      "       2    6   57\n",
      "\n",
      "Average Accuracy: 0.9215686274509803\n",
      "Macro Average f-measure: 0.9190476190476191 \n",
      "\n",
      "\n",
      "  actual    1    2\n",
      "--------  ---  ---\n",
      "       1   76    9\n",
      "       2    9   59\n",
      "\n",
      "Average Accuracy: 0.8823529411764706\n",
      "Macro Average f-measure: 0.8808823529411764 \n",
      "\n",
      "\n",
      "  actual    1    2\n",
      "--------  ---  ---\n",
      "       1   80    5\n",
      "       2    7   61\n",
      "\n",
      "Average Accuracy: 0.9215686274509803\n",
      "Macro Average f-measure: 0.9191176470588236 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BIN SIZE 1\n",
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, auto.columns())\n",
    "\n",
    "# Discretize mpg values with three equal-width bins\n",
    "discretize(auto, 'mpg', [22])\n",
    "\n",
    "# Normalize all the columns\n",
    "norm_cols = ['cyls','disp','hp','weight','accl']\n",
    "for column in norm_cols:\n",
    "    normalize(auto, column)\n",
    "    \n",
    "# Run 3 iterations of knn\n",
    "for i in range(3):\n",
    "\n",
    "    # Create test and train set with holdout\n",
    "    test_set_size = int(auto.row_count() / 2)\n",
    "    train_set, test_set = holdout(auto, test_set_size)\n",
    "\n",
    "    # knn evaluation\n",
    "    confusion_matrix = knn_eval(train_set, test_set, majority_vote, 5, 'mpg', ['cyls','weight','accl'])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Create a list of accuracies for each mpg label\n",
    "    acc = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "    # Calculate the macro average f-measure\n",
    "    f_measures = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        cur_recall = recall(confusion_matrix, label)\n",
    "        cur_precision = recall(confusion_matrix, label)\n",
    "        f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "    print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa37de14",
   "metadata": {},
   "source": [
    "Bin Size 2: [15, 20, 25, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cc521f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    2    1    3    4    5\n",
      "--------  ---  ---  ---  ---  ---\n",
      "       2   26    8   19    1    0\n",
      "       1    9   16    0    0    0\n",
      "       3    5    0   18    7    0\n",
      "       4    1    0    8   12    7\n",
      "       5    0    0    1    3   12\n",
      "\n",
      "Average Accuracy: 0.819607843137255\n",
      "Macro Average f-measure: 0.580010582010582 \n",
      "\n",
      "\n",
      "  actual    2    1    3    4    5\n",
      "--------  ---  ---  ---  ---  ---\n",
      "       2   29   14    2    0    0\n",
      "       1    6   14    0    0    0\n",
      "       3   17    1   10    3    1\n",
      "       4    3    0   13   14    6\n",
      "       5    0    0    0    9   11\n",
      "\n",
      "Average Accuracy: 0.803921568627451\n",
      "Macro Average f-measure: 0.5191666666666667 \n",
      "\n",
      "\n",
      "  actual    2    1    3    4    5\n",
      "--------  ---  ---  ---  ---  ---\n",
      "       2   31    9    5    1    0\n",
      "       1   11   17    0    0    0\n",
      "       3    8    1   12    7    0\n",
      "       4    1    0    9   11   13\n",
      "       5    0    0    0    3   14\n",
      "\n",
      "Average Accuracy: 0.8222222222222223\n",
      "Macro Average f-measure: 0.5713372305443917 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BIN SIZE 2\n",
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, auto.columns())\n",
    "\n",
    "# Discretize mpg values with three equal-width bins\n",
    "discretize(auto, 'mpg', [15, 20, 25, 30])\n",
    "\n",
    "# Normalize all the columns\n",
    "norm_cols = ['cyls','disp','hp','weight','accl']\n",
    "for column in norm_cols:\n",
    "    normalize(auto, column)\n",
    "    \n",
    "# Run 3 iterations of knn\n",
    "for i in range(3):\n",
    "\n",
    "    # Create test and train set with holdout\n",
    "    test_set_size = int(auto.row_count() / 2)\n",
    "    train_set, test_set = holdout(auto, test_set_size)\n",
    "\n",
    "    # knn evaluation\n",
    "    confusion_matrix = knn_eval(train_set, test_set, majority_vote, 5, 'mpg', ['cyls','weight','accl'])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Create a list of accuracies for each mpg label\n",
    "    acc = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "    # Calculate the macro average f-measure\n",
    "    f_measures = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        cur_recall = recall(confusion_matrix, label)\n",
    "        cur_precision = recall(confusion_matrix, label)\n",
    "        f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "    print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a7d89a",
   "metadata": {},
   "source": [
    "11. Pick a bin size and voting method from above, and redo 6 but with two different k values (i.e., add two more for loops).\n",
    "\n",
    "k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37ea5646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   69    9    0\n",
      "       2   10   42    3\n",
      "       3    0   13    7\n",
      "\n",
      "Average Accuracy: 0.8474945533769063\n",
      "Macro Average f-measure: 0.6660839160839161 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   65   10    0\n",
      "       2   11   42    8\n",
      "       3    0    4   13\n",
      "\n",
      "Average Accuracy: 0.8562091503267973\n",
      "Macro Average f-measure: 0.7732990463945141 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   67    5    0\n",
      "       2   18   38    8\n",
      "       3    0    5   12\n",
      "\n",
      "Average Accuracy: 0.8431372549019608\n",
      "Macro Average f-measure: 0.7433959694989106 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k = 3\n",
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, auto.columns())\n",
    "\n",
    "# Discretize mpg values with three equal-width bins\n",
    "discretize(auto, 'mpg', [20,30])\n",
    "\n",
    "# Normalize all the columns\n",
    "norm_cols = ['cyls','disp','hp','weight','accl']\n",
    "for column in norm_cols:\n",
    "    normalize(auto, column)\n",
    "    \n",
    "# Run 3 iterations of knn\n",
    "for i in range(3):\n",
    "\n",
    "    # Create test and train set with holdout\n",
    "    test_set_size = int(auto.row_count() / 2)\n",
    "    train_set, test_set = holdout(auto, test_set_size)\n",
    "\n",
    "    # knn evaluation\n",
    "    confusion_matrix = knn_eval(train_set, test_set, majority_vote, 3, 'mpg', ['cyls','weight','accl'])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Create a list of accuracies for each mpg label\n",
    "    acc = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "    # Calculate the macro average f-measure\n",
    "    f_measures = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        cur_recall = recall(confusion_matrix, label)\n",
    "        cur_precision = recall(confusion_matrix, label)\n",
    "        f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "    print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cb2acd",
   "metadata": {},
   "source": [
    "k = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c728d79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   61    9    0\n",
      "       2    7   53    2\n",
      "       3    0   12    9\n",
      "\n",
      "Average Accuracy: 0.869281045751634\n",
      "Macro Average f-measure: 0.7182795698924731 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   65   12    0\n",
      "       2   10   43    1\n",
      "       3    0   14    8\n",
      "\n",
      "Average Accuracy: 0.8387799564270152\n",
      "Macro Average f-measure: 0.6680295013628347 \n",
      "\n",
      "\n",
      "  actual    1    2    3\n",
      "--------  ---  ---  ---\n",
      "       1   72    4    0\n",
      "       2   15   41    6\n",
      "       3    0    7    8\n",
      "\n",
      "Average Accuracy: 0.8605664488017428\n",
      "Macro Average f-measure: 0.7139973589888701 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# k = 6\n",
    "auto = DataTable(['mpg','cyls','disp','hp','weight','accl','year','origin','name'])\n",
    "auto.load('auto-mpg.txt')\n",
    "auto = remove_duplicates(auto)\n",
    "auto = remove_missing(auto, auto.columns())\n",
    "\n",
    "# Discretize mpg values with three equal-width bins\n",
    "discretize(auto, 'mpg', [20,30])\n",
    "\n",
    "# Normalize all the columns\n",
    "norm_cols = ['cyls','disp','hp','weight','accl']\n",
    "for column in norm_cols:\n",
    "    normalize(auto, column)\n",
    "    \n",
    "# Run 3 iterations of knn\n",
    "for i in range(3):\n",
    "\n",
    "    # Create test and train set with holdout\n",
    "    test_set_size = int(auto.row_count() / 2)\n",
    "    train_set, test_set = holdout(auto, test_set_size)\n",
    "\n",
    "    # knn evaluation\n",
    "    confusion_matrix = knn_eval(train_set, test_set, majority_vote, 3, 'mpg', ['cyls','weight','accl'])\n",
    "    print(confusion_matrix)\n",
    "\n",
    "    # Create a list of accuracies for each mpg label\n",
    "    acc = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        acc.append(accuracy(confusion_matrix, label))\n",
    "\n",
    "    print(\"\\nAverage Accuracy:\", sum(acc)/len(acc))\n",
    "\n",
    "    # Calculate the macro average f-measure\n",
    "    f_measures = []\n",
    "    for label in distinct_values(auto, ['mpg']):\n",
    "        cur_recall = recall(confusion_matrix, label)\n",
    "        cur_precision = recall(confusion_matrix, label)\n",
    "        f_measures.append((2 * cur_precision * cur_recall) / (cur_recall + cur_precision))\n",
    "\n",
    "    print(\"Macro Average f-measure:\", sum(f_measures)/len(f_measures), \"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad6390bb",
   "metadata": {},
   "source": [
    "# 3. Issues, Challenges, and Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf6b940",
   "metadata": {},
   "source": [
    "One of the most difficult challenges that I faced was trying to keep all of the indexes for the confusion matrix in order. Given that there were so many rows and column indexes, it was difficult to design the evaluation algorithm for that reason. Similarly, specific DataTable aspects created errors that caused my program to fail even though the concept was correct. Other than that, this homework went pretty well.\n",
    "\n",
    "I thought that it was interesting that the weighted voting did slightly worse than majority voting as I thought it would provide more insight to the Euclidean distances of each row in knn. Moreover, it was apparent that bins that created less labels would have a higher accuracy as they do not need to choose from as many different options. Lastly, the k values chosen to change the evaluation were not significant enough to note. I think a substantial amount of k would be needed to really show a difference, especially given the size of the dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
